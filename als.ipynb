{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1d52a7",
   "metadata": {},
   "source": [
    "# Applying ALS methods as collaborative filtering approach for yelp dataset\n",
    "In the follwoing, we use review dataset of yelp to create the sparse matrix and apply ALS approach to recommend restaurant to the users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4779e",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f4ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "#add the following two in order to use avg\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10abd44d",
   "metadata": {},
   "source": [
    "## Initializing Spark to use its dataframe as the main datastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058105af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a spark session.\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd94da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark() # Initializate spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9456a",
   "metadata": {},
   "source": [
    "## Reading data from the source files\n",
    "In this part, we use review, user, business dataset. The main dataset that we are working on is the review.json but we use other two datasets to preprocess the data. With business dataset, we filter the businesses which are related to the restaurants and with the user dataset, we filter the users which contains less than 10 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9cf174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the review dataset and spilling on pyspark dataframe\n",
    "path = 'data/yelp_academic_dataset_review.json'\n",
    "df_review = spark.read.json(path).limit(500000)\n",
    "# df_review = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7596f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the buisiness dataset and spilling on pyspark dataframe\n",
    "path = 'data/yelp_academic_dataset_business.json'\n",
    "df_business = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66bd856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the user dataset and spilling on pyspark dataframe\n",
    "path = 'data/yelp_academic_dataset_user.json'\n",
    "df_user = spark.read.json(path)\n",
    "df_user = df_user.select('user_id', 'review_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a277670",
   "metadata": {},
   "source": [
    "### 1. filter the users with more than 10 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f3781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the users with more than 10 review counts\n",
    "df_user = df_user.filter(col('review_count')>=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12c5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need userId, businessId and ratings for ALS\n",
    "df_review = df_review[['user_id','business_id','stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "448c2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply on review dataset\n",
    "df_review = df_review.join(df_user, df_review.user_id == df_user.user_id ,\"left_semi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734809b5",
   "metadata": {},
   "source": [
    "### 2. Filter the restaurant businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "104215a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter restaurant businesses \n",
    "df_business = df_business.filter(df_business.categories.contains('Restaurants'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4bc0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = df_review.join(df_business[['business_id']], \n",
    "                             df_review.business_id == df_business.business_id ,\"left_semi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b2131",
   "metadata": {},
   "source": [
    "Dropping out these two DataFrame to manage memory and speedup the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97442bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: string, review_count: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_business.unpersist()\n",
    "df_user.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf45f36",
   "metadata": {},
   "source": [
    "### 3. ALS works with user and business ID in numerical format.\n",
    "So here we are converting the user and business ID to the numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48872418",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df_review) \n",
    "            for column in list(set(df_review.columns)-set(['stars'])) ]\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_review = pipeline.fit(df_review).transform(df_review)\n",
    "\n",
    "# df_review.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a8c6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146462\n",
      "255679\n"
     ]
    }
   ],
   "source": [
    "print(df_review.select('user_id').distinct().count())\n",
    "print(df_review.select('user_id').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d3006",
   "metadata": {},
   "source": [
    "### spliting the dataset and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2993e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_train, df_test) = df_review.randomSplit([0.8, 0.2], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46daae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"stars\", predictionCol=\"glb_average\")\n",
    "eval_mae = RegressionEvaluator(metricName=\"mae\", labelCol=\"stars\", predictionCol=\"glb_average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db8a412c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for global average = 1.248\n",
      "MAE for global average = 1.003\n"
     ]
    }
   ],
   "source": [
    "glb_average = df_train.select(avg('stars')).collect()[0][0] \n",
    "df_test = df_test.withColumn('glb_average', lit(glb_average))\n",
    "rmse = eval_rmse.evaluate(df_test)\n",
    "mae = eval_mae.evaluate(df_test)\n",
    "print(\"RMSE for global average = %.3f\"%rmse)\n",
    "print(\"MAE for global average = %.3f\"%mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538cba50",
   "metadata": {},
   "source": [
    "We use validation in training set to choose the best set of hyperparameters for the dataset. Then we apply the best set to test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "243e22e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"stars\", predictionCol=\"prediction\")\n",
    "eval_mae = RegressionEvaluator(metricName=\"mae\", labelCol=\"stars\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b890f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create evaluator object\n",
    "maxIter = [17]\n",
    "regParam = [0.4,0.6]\n",
    "rank = [10,20,50]\n",
    "#create the als model\n",
    "als = ALS(userCol=\"user_id_index\", \n",
    "          itemCol=\"business_id_index\", \n",
    "          ratingCol=\"stars\",seed=0, nonnegative=True, coldStartStrategy=\"drop\")\n",
    "\n",
    "#we use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "#trainValidationSplit will try all combinations of values and determine best model using the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(als.regParam, regParam) \\\n",
    "    .addGrid(als.maxIter, maxIter)\\\n",
    "    .addGrid(als.rank, rank)\\\n",
    "    .build()\n",
    "\n",
    "#in this case the estimator is ALS.\n",
    "#a TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=als,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=eval_rmse,\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99d552cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the model = 1.409\n",
      "MAE for the model = 1.125\n"
     ]
    }
   ],
   "source": [
    "#make predictions on test data. model is the model with combination of parameters\n",
    "#that performed best.\n",
    "prediction = model.transform(df_test)\n",
    "rmse = eval_rmse.evaluate(prediction)\n",
    "mae = eval_mae.evaluate(prediction)\n",
    "print(\"RMSE for the model = %.3f\"%rmse)\n",
    "print(\"MAE for the model = %.3f\"%mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
